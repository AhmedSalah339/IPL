{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn import base\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-03T21:02:34.907409Z","iopub.execute_input":"2021-08-03T21:02:34.90787Z","iopub.status.idle":"2021-08-03T21:02:34.925497Z","shell.execute_reply.started":"2021-08-03T21:02:34.90783Z","shell.execute_reply":"2021-08-03T21:02:34.924067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_df = pd.read_csv('/kaggle/input/ipl-features-eng/matches_fat_eng.csv')\nfeat_df = feat_df.iloc[:,1:]\ntarget = 'winner'\ntest_size =  0.2\ncols_to_drop = ['date','id','result','dl_applied','win_by_runs','win_by_wickets','player_of_match']","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:28:10.797972Z","iopub.execute_input":"2021-08-03T21:28:10.798489Z","iopub.status.idle":"2021-08-03T21:28:10.825579Z","shell.execute_reply.started":"2021-08-03T21:28:10.798455Z","shell.execute_reply":"2021-08-03T21:28:10.824386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_df_training = feat_df.drop(columns=cols_to_drop)\nfeat_df_training = feat_df_training.fillna(0,axis=0)\nfeat_df_training['population'] = feat_df_training['population']/10**6\nfeat_df_training.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:28:11.05732Z","iopub.execute_input":"2021-08-03T21:28:11.057683Z","iopub.status.idle":"2021-08-03T21:28:11.093174Z","shell.execute_reply.started":"2021-08-03T21:28:11.057653Z","shell.execute_reply":"2021-08-03T21:28:11.091945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_x_y(df,target):\n    X,y = df.drop(columns=target),df[target]\n    return X,y","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:04:00.770931Z","iopub.execute_input":"2021-08-03T21:04:00.771576Z","iopub.status.idle":"2021-08-03T21:04:00.776526Z","shell.execute_reply.started":"2021-08-03T21:04:00.771525Z","shell.execute_reply":"2021-08-03T21:04:00.77525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> As we have a small dataset we have to make sure that the percentage of positive samples are equal across all the splits (train/dev/test)","metadata":{}},{"cell_type":"code","source":"def train_test_split_df(df,target,test_season,start_season=None):\n    train = df[df['Season']<test_season]\n    if start_season != None:\n        train = train[train['Season']>start_season-1]\n    test = df[df['Season']==test_season]\n    X_train,y_train = get_x_y(train,target)\n    X_test,y_test = get_x_y(test,target)\n    return X_train,y_train,X_test,y_test","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:02:35.055319Z","iopub.execute_input":"2021-08-03T21:02:35.055794Z","iopub.status.idle":"2021-08-03T21:02:35.06984Z","shell.execute_reply.started":"2021-08-03T21:02:35.055764Z","shell.execute_reply":"2021-08-03T21:02:35.068838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_test_split(df,target,test_size):\n#     positive_examples = df[df[target]== 1]\n#     negative_examples = df[df[target] == 0]\n    \n#     train_p,test_p = train_test_split_df(positive_examples,test_size)\n#     train_n,test_n = train_test_split_df(negative_examples,test_size)\n    \n#     train = pd.concat([train_p, train_n], ignore_index=True)\n#     test = pd.concat([test_p, test_n], ignore_index=True)\n#     return train,test\n ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:02:35.071611Z","iopub.execute_input":"2021-08-03T21:02:35.072411Z","iopub.status.idle":"2021-08-03T21:02:35.084839Z","shell.execute_reply.started":"2021-08-03T21:02:35.072361Z","shell.execute_reply":"2021-08-03T21:02:35.083963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,y_train,X_test,y_test = train_test_split_df(feat_df_training,target,2019)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:29:32.481866Z","iopub.execute_input":"2021-08-03T21:29:32.482306Z","iopub.status.idle":"2021-08-03T21:29:32.49194Z","shell.execute_reply.started":"2021-08-03T21:29:32.482268Z","shell.execute_reply":"2021-08-03T21:29:32.490548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> now let's implement cross validation using the split method above","metadata":{}},{"cell_type":"code","source":"def roll_over(df,target,model,loss_func,start_season,isclone=True):\n    seas = start_season\n    end_season = start_season+1\n    final_season = max(df.Season)\n    losses = []\n    while end_season <final_season:\n        end_season +=1 \n        X_train,y_train,X_test,y_test = train_test_split_df(df,target,end_season,seas)\n        model.fit(X_train,y_train)\n        pred = model.predict_proba(X_test)[:, 1]\n#         print(pred)\n        losses.append(loss_func(y_test,pred))\n        if isclone:\n            model = base.clone(model)\n        seas+=1 \n    return np.average(losses)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:23:19.820784Z","iopub.execute_input":"2021-08-03T22:23:19.821169Z","iopub.status.idle":"2021-08-03T22:23:19.829076Z","shell.execute_reply.started":"2021-08-03T22:23:19.821138Z","shell.execute_reply":"2021-08-03T22:23:19.827817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0,solver='liblinear')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:23:20.453431Z","iopub.execute_input":"2021-08-03T22:23:20.453787Z","iopub.status.idle":"2021-08-03T22:23:20.458048Z","shell.execute_reply.started":"2021-08-03T22:23:20.453757Z","shell.execute_reply":"2021-08-03T22:23:20.45726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nloss_func = log_loss","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:23:20.894798Z","iopub.execute_input":"2021-08-03T22:23:20.895387Z","iopub.status.idle":"2021-08-03T22:23:20.899507Z","shell.execute_reply.started":"2021-08-03T22:23:20.895347Z","shell.execute_reply":"2021-08-03T22:23:20.898399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roll_over(feat_df_training,target,clf,loss_func,2008)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:23:21.653414Z","iopub.execute_input":"2021-08-03T22:23:21.653816Z","iopub.status.idle":"2021-08-03T22:23:21.811725Z","shell.execute_reply.started":"2021-08-03T22:23:21.653781Z","shell.execute_reply":"2021-08-03T22:23:21.810201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nclf = lgb.LGBMClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:23:22.408534Z","iopub.execute_input":"2021-08-03T22:23:22.408928Z","iopub.status.idle":"2021-08-03T22:23:22.41334Z","shell.execute_reply.started":"2021-08-03T22:23:22.408896Z","shell.execute_reply":"2021-08-03T22:23:22.412322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roll_over(feat_df_training,target,clf,loss_func,2008)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:23:23.122546Z","iopub.execute_input":"2021-08-03T22:23:23.123111Z","iopub.status.idle":"2021-08-03T22:23:23.542781Z","shell.execute_reply.started":"2021-08-03T22:23:23.12306Z","shell.execute_reply":"2021-08-03T22:23:23.541663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Base_Model:\n    def __init__(self):\n        self.X_train = None\n        self.y_train = None\n    def fit(self,X_train,y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n    def predict_proba(self,X_test):\n        df = self.X_train\n        df['target'] = self.y_train\n        probs = []\n        for idx,row in X_test.iterrows():\n            d = row['duals_sets']\n            seas = row['Season']\n#             print((d,seas))\n            df_seas = df[(df['Season']==seas-1) & (df['duals_sets']==d)]\n            if df_seas.shape[0]>0:\n                probs.append([0,df_seas[df_seas['target']==1].shape[0]/df_seas.shape[0]])\n            else:\n                probs.append([0,0.5])\n#         print(probs)\n        return np.array(probs)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:28:23.825688Z","iopub.execute_input":"2021-08-03T22:28:23.826075Z","iopub.status.idle":"2021-08-03T22:28:23.83639Z","shell.execute_reply.started":"2021-08-03T22:28:23.826041Z","shell.execute_reply":"2021-08-03T22:28:23.834769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roll_over(feat_df_training,target,Base_Model(),loss_func,2008,False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:28:24.121399Z","iopub.execute_input":"2021-08-03T22:28:24.12197Z","iopub.status.idle":"2021-08-03T22:28:25.265262Z","shell.execute_reply.started":"2021-08-03T22:28:24.121924Z","shell.execute_reply":"2021-08-03T22:28:25.264005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Base_Model_2:\n    def __init__(self):\n        self.X_train = None\n        self.y_train = None\n    def fit(self,X_train,y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n    def predict_proba(self,X_test):\n        df = self.X_train\n        df['target'] = self.y_train\n        probs = []\n        for idx,row in X_test.iterrows():\n            probs.append([0,0.5])\n#         print(probs)\n        return np.array(probs)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:29:10.429223Z","iopub.execute_input":"2021-08-03T22:29:10.429848Z","iopub.status.idle":"2021-08-03T22:29:10.438024Z","shell.execute_reply.started":"2021-08-03T22:29:10.429805Z","shell.execute_reply":"2021-08-03T22:29:10.436463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roll_over(feat_df_training,target,Base_Model_2(),loss_func,2008,False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T22:29:10.706867Z","iopub.execute_input":"2021-08-03T22:29:10.707283Z","iopub.status.idle":"2021-08-03T22:29:10.818399Z","shell.execute_reply.started":"2021-08-03T22:29:10.707244Z","shell.execute_reply":"2021-08-03T22:29:10.816982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> So, by just predecting 0.5 prob every time we can get 0.69 log loss","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'Value':clf.feature_importances_,'Feature':clf.feature_name_}).sort_values(by=\"Value\",ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T21:53:12.363163Z","iopub.execute_input":"2021-08-03T21:53:12.363751Z","iopub.status.idle":"2021-08-03T21:53:12.386508Z","shell.execute_reply.started":"2021-08-03T21:53:12.363702Z","shell.execute_reply":"2021-08-03T21:53:12.385379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We can get rid of all ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}